# [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)

This repository contains code and human annotation data for **measuring and quantifying self-bias in LLM-as-a-judge** evaluations, as described in our paper.

📄 **Paper:** [https://arxiv.org/abs/2508.06709](https://arxiv.org/abs/2508.06709)

## 📄 Overview
Large language models (LLMs) can act as judges for evaluating other LLM outputs. However, they may systematically give overly favorable ratings to their own outputs — a phenomenon known as **self-bias** — which can distort performance assessments.

Our paper introduces a **statistical framework** that:
- Explicitly models the conditions under which self-bias can be detected and measured.
- Accounts for genuine quality differences of models and separates them from self-bias, using an independent third-party judge (e.g., humans).
- Accounts for consistent annotator differences (across all models), which are independent of self-bias.

Through an empirical study of over **5000 prompt–completion pairs** rated by both humans and nine different LLM judges, we show:
- Certain models (e.g., GPT-4o, Claude 3.5 Sonnet) exhibit **systematic self-bias**.
- Models also show **family-bias** — favoring outputs from models in the same family.
- We offer **practical guidance** to mitigate these biases in automated evaluation pipelines.
- Finally, we **release** the human annotations, to facilitate future research.

---

## 📂 Repository Structure
```
project/
├─ data/                      # CSV input files (e.g., Completeness.csv, Helpfulness.csv, ...)
├─ plots/                     # Figures generated by the analysis
├─ regression.R               # Original R script to replicate the statistical analysis
├─ requirements.txt           # Python dependencies
└─ code_python/               # Python source code
   ├─ config.py               # Paths, constants, model/family orders
   ├─ data_prep.py            # Data loading, cleaning, preprocessing, length features
   ├─ stats_models.py         # Statistical models, robust OLS, ordinal/logit, GAMs
   ├─ viz.py                  # Plotting and visualization helpers
   └─ main_analysis.py        # Master script to run the analysis
```

---

## ⚙️ Setup
1. Install Python 3.9+ and `pip`.
2. From the project root, install dependencies:
```bash
pip install -r requirements.txt
```

---

## ▶️ Running the Analysis
Run the main Python script from the project root so relative imports work correctly:
```bash
python -m code_python.main_analysis
```

To run the original R analysis, execute:
```r
Rscript regression.R
```

---

## 📊 Data Requirements
Each CSV in `data/` must contain the columns:
- `judge`
- `model`
- `gt`
- `dataset`
- `prompt_id`
- `rating`
- `pred_length`

The filenames must match the dimension names in `config.py`’s `DEFAULT_DIMENSIONS` list.

---

## 📈 Output
- Figures are saved to the `plots/` directory in PDF format.
- Plots include heatmaps, correlation charts, and bias estimation charts.

---

## 📚 Citation
If you use this code or data, please cite our paper:

```bibtex
@article{spiliopoulou2025play,
  title={Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge},
  author={Spiliopoulou, Evangelia and Fogliato, Riccardo and Burnsky, Hanna and Soliman, Tamer and Ma, Jie and Horwood, Graham and Ballesteros, Miguel},
  journal={arXiv preprint arXiv:2508.06709},
  year={2025}
}
```

---
📄 **Paper:** [https://arxiv.org/abs/2508.06709](https://arxiv.org/abs/2508.06709)
